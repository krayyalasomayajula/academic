{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252979ba-05f9-4002-9a14-1f4e502f60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c1949-80df-4421-b1ec-a1e3049e4811",
   "metadata": {},
   "source": [
    "# Load `csv` config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ca1843-a901-47ba-80de-7ed0310613f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Image', 'Word', 'Writer'], dtype='object')\n",
      "Sample train/valid labels\n",
      "Create train datasets\n",
      "Create valid datasets\n",
      "Save train/valid csv files\n"
     ]
    }
   ],
   "source": [
    "in_dirs = ['/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data/IAM_train']\n",
    "csv_files = ['/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data/IAM_train.csv']\n",
    "out_dirs = ['/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data_3_2_by5']\n",
    "\n",
    "\n",
    "SAMPLE_SIZE_TR, SAMPLE_SIZE_VD = 3, 2\n",
    "SAMPLE_SIZE = SAMPLE_SIZE_TR + SAMPLE_SIZE_VD\n",
    "\n",
    "    \n",
    "for i_dir, f_csv, o_dir in zip(in_dirs, csv_files, out_dirs):\n",
    "    df_input = pd.read_csv(f_csv, names=None)\n",
    "    print(df_input.columns)\n",
    "    df_train = pd.DataFrame()\n",
    "    df_valid = pd.DataFrame()\n",
    "    values, counts = np.unique(list(df_input['Word']), return_counts=True)\n",
    "    assert np.min(counts) >= SAMPLE_SIZE, \"Error: Not enough samples\"\n",
    "    groups = df_input.groupby('Word')\n",
    "    \n",
    "    print('Sample train/valid labels')\n",
    "    #pdb.set_trace()\n",
    "    for key, grp in groups:\n",
    "        #print(f'{key}, {len(grp)}')\n",
    "        df_sample = grp.sample(n=SAMPLE_SIZE).reset_index(drop=True)\n",
    "        df_train = pd.concat([df_train, df_sample.iloc[:SAMPLE_SIZE_TR]], ignore_index=True)\n",
    "        df_valid = pd.concat([df_valid, df_sample.iloc[SAMPLE_SIZE_TR:]], ignore_index=True) #.reset_index(drop=True)])\n",
    "\n",
    "    print('Remove stale samples')\n",
    "    if os.path.exists(o_dir):\n",
    "        shutil.rmtree(o_dir)\n",
    "    \n",
    "    tr_dir = os.path.join(o_dir, 'train')\n",
    "    vd_dir = os.path.join(o_dir, 'valid')\n",
    "    if not os.path.exists(tr_dir):\n",
    "        os.makedirs(tr_dir)\n",
    "    if not os.path.exists(vd_dir):\n",
    "        os.makedirs(vd_dir)\n",
    "    \n",
    "    print('Create train datasets')\n",
    "    #pdb.set_trace()\n",
    "    f_train = i_dir + '/' + df_train['Image']\n",
    "    result = map(lambda fil: shutil.copy(fil, tr_dir), f_train)\n",
    "    xx = list(result) # this actually copies maybe something to do with copy buffer\n",
    "    \n",
    "    print('Create valid datasets')\n",
    "    #pdb.set_trace()\n",
    "    f_valid = i_dir + '/' + df_valid['Image']\n",
    "    result = map(lambda fil: shutil.copy(fil, vd_dir), f_valid)\n",
    "    xx = list(result) # this actually copies maybe something to do with copy buffer\n",
    "    \n",
    "    print('Save train/valid csv files')\n",
    "    df_train.to_csv(os.path.join(o_dir, 'iam_by5_train.csv'), index=False)\n",
    "    df_valid.to_csv(os.path.join(o_dir, 'iam_by5_valid.csv'), index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
