{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252979ba-05f9-4002-9a14-1f4e502f60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c1949-80df-4421-b1ec-a1e3049e4811",
   "metadata": {},
   "source": [
    "# CLI config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ca1843-a901-47ba-80de-7ed0310613f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_log directory: exists\n",
      "dict_keys(['device', 'global_macros', 'loaders', 'model', 'criterion', 'metric', 'trainer'])\n"
     ]
    }
   ],
   "source": [
    "cli = {}\n",
    "cli['config'] = './config/zs_config.yml'\n",
    "cli['log_dir'] = './test_log'\n",
    "\n",
    "if cli['log_dir'] is None:\n",
    "        cli['log_dir'] = input (\"Enter directory to save model and logs:\")\n",
    "    \n",
    "if not os.path.exists(cli['log_dir']):\n",
    "    os.makedirs(cli['log_dir'])\n",
    "else:\n",
    "    print(f\"{cli['log_dir']} directory: exists\")\n",
    "\n",
    "with open(cli['config'], 'r') as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "print(args.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bf1a51-326a-47a7-ace7-8e07fdc1d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (_NATIVE_AMP_AVAILABLE, _TORCHVISION_AVAILABLE,\n",
    "                  _GYM_AVAILABLE, _SKLEARN_AVAILABLE,\n",
    "                  _PIL_AVAILABLE, _OPENCV_AVAILABLE,\n",
    "                  _WANDB_AVAILABLE, _MATPLOTLIB_AVAILABLE,\n",
    "                  _TORCHVISION_LESS_THAN_0_9_1, _PL_GREATER_EQUAL_1_4,\n",
    "                  _PL_GREATER_EQUAL_1_4_5, _TORCH_ORT_AVAILABLE,\n",
    "                  _TORCH_MAX_VERSION_SPARSEML, _SPARSEML_AVAILABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319e3a42-398e-4bc4-9b81-cd951ee8d6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_NATIVE_AMP_AVAILABLE: True\n",
      "_TORCHVISION_AVAILABLE: True\n",
      "_GYM_AVAILABLE: False\n",
      "_SKLEARN_AVAILABLE: True\n",
      "_PIL_AVAILABLE: True\n",
      "_OPENCV_AVAILABLE: False\n",
      "_WANDB_AVAILABLE: True\n",
      "_MATPLOTLIB_AVAILABLE: True\n",
      "_TORCHVISION_LESS_THAN_0_9_1: False\n",
      "_PL_GREATER_EQUAL_1_4: True\n",
      "_PL_GREATER_EQUAL_1_4_5: True\n",
      "_TORCH_ORT_AVAILABLE: False\n",
      "_TORCH_MAX_VERSION_SPARSEML: False\n",
      "_SPARSEML_AVAILABLE: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"_NATIVE_AMP_AVAILABLE: {_NATIVE_AMP_AVAILABLE}\")\n",
    "\n",
    "print(f\"_TORCHVISION_AVAILABLE: {_TORCHVISION_AVAILABLE}\")\n",
    "print(f\"_GYM_AVAILABLE: {_GYM_AVAILABLE}\")\n",
    "print(f\"_SKLEARN_AVAILABLE: {_SKLEARN_AVAILABLE}\")\n",
    "print(f\"_PIL_AVAILABLE: {_PIL_AVAILABLE}\")\n",
    "print(f\"_OPENCV_AVAILABLE: {_OPENCV_AVAILABLE}\")\n",
    "print(f\"_WANDB_AVAILABLE: {_WANDB_AVAILABLE}\")\n",
    "print(f\"_MATPLOTLIB_AVAILABLE: {_MATPLOTLIB_AVAILABLE}\")\n",
    "print(f\"_TORCHVISION_LESS_THAN_0_9_1: {_TORCHVISION_LESS_THAN_0_9_1}\")\n",
    "print(f\"_PL_GREATER_EQUAL_1_4: {_PL_GREATER_EQUAL_1_4}\")\n",
    "print(f\"_PL_GREATER_EQUAL_1_4_5: {_PL_GREATER_EQUAL_1_4_5}\")\n",
    "print(f\"_TORCH_ORT_AVAILABLE: {_TORCH_ORT_AVAILABLE}\")\n",
    "print(f\"_TORCH_MAX_VERSION_SPARSEML: {_TORCH_MAX_VERSION_SPARSEML}\")\n",
    "print(f\"_SPARSEML_AVAILABLE: {_SPARSEML_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9422d203-cb08-4893-b17e-49bd023cbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#from pl_bolts.utils import _PIL_AVAILABLE\n",
    "#from pl_bolts.utils.warnings import warn_missing_pkg\n",
    "\n",
    "if _PIL_AVAILABLE:\n",
    "    from PIL import Image\n",
    "else:  # pragma: no cover\n",
    "    print(f\"Warning missing pkg: Pillow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119e0121-d751-4e8c-b2d7-05b693094f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_config': {'general': {'ds_class': ['utils.dataset.phosc.PHOSCZSDataset', 'utils.dataset.phosc.ImageDataset', 'utils.dataset.phosc.PhosDataset', 'utils.dataset.phosc.PhocDataset', 'utils.dataset.phosc.WordlabelDataset'], 'type': 'dir', 'raw': None, 'label': None, 'unseen': None}, 'train': {'raw': '/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data_3_2_by5/train', 'label': '/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data_3_2_by5/iam_by5_train.csv'}, 'valid': {'raw': '/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data_3_2_by5/valid', 'label': '/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data_3_2_by5/iam_by5_valid.csv'}}, 'loader_config': {'utils.dataloader.phosc.PHOSCZSDataModule': {'seed': 1234, 'drop_last': True, 'pin_memory': True, 'shuffle': True, 'batch_size': 64, 'num_workers': 10}}}\n",
      "dict_keys(['dataset_config', 'loader_config'])\n"
     ]
    }
   ],
   "source": [
    "print(args['loaders'])\n",
    "print(args['loaders'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc139373-821e-4e01-9354-7639025d0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generic.parsing import recursive_parse_settings\n",
    "from utils.generic.getaccess import get_class_from_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "923baec6-4431-4d92-a5ac-0d70bf8d3b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'ds_class': ['utils.dataset.phosc.PHOSCZSDataset', 'utils.dataset.phosc.ImageDataset', 'utils.dataset.phosc.PhosDataset', 'utils.dataset.phosc.PhocDataset', 'utils.dataset.phosc.WordlabelDataset'], 'type': 'dir', 'raw': None, 'label': None, 'unseen': None}, 'train': {'raw': '/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data_3_2_by5/train', 'label': '/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data_3_2_by5/iam_by5_train.csv'}, 'valid': {'raw': '/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data_3_2_by5/valid', 'label': '/media/windowsC/SIN_Desktop/projects/ZSL_WordSpotting/IAM_Data_3_2_by5/iam_by5_valid.csv'}}\n",
      "{'utils.dataloader.phosc.PHOSCZSDataModule': {'seed': 1234, 'drop_last': True, 'pin_memory': True, 'shuffle': True, 'batch_size': 64, 'num_workers': 10}}\n"
     ]
    }
   ],
   "source": [
    "dataset_config = args['loaders']['dataset_config']\n",
    "loader_config = args['loaders']['loader_config']\n",
    "print(dataset_config)\n",
    "print(loader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed1903c9-143a-412d-9bef-74dc2b2ed01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(trainset): 15219, size(validset): 10146\n"
     ]
    }
   ],
   "source": [
    "loader_key, loader_config_value = next(iter(loader_config.items()))\n",
    "loader_class = get_class_from_str(loader_key)\n",
    "phosc_loader = loader_class(dataset_config, loader_config_value)\n",
    "phosc_loader.prepare_data()\n",
    "\n",
    "trainset_sz = len(phosc_loader.trainset)\n",
    "validset_sz = len(phosc_loader.validset)\n",
    "print(f\"size(trainset): {trainset_sz}, size(validset): {validset_sz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb40ac1-d96f-4d32-9d20-d791f66e641d",
   "metadata": {},
   "source": [
    "# Test <code>dataset</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df1a459-d89d-4938-b243-66b0feb0b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_data_from_dataset(dataset, n_epochs, steps_per_epoch, batch_size):\n",
    "    for epoch_idx in range(n_epochs):\n",
    "        for batch_idx in range(steps_per_epoch):\n",
    "            batch_dict = {}\n",
    "            for idx in range(batch_size):\n",
    "                sample = next(iter(dataset))\n",
    "                if len(batch_dict) ==0 :\n",
    "                    for key, value in sample.items():\n",
    "                        batch_dict[key] = [value]\n",
    "                else:\n",
    "                    for key, value in sample.items():\n",
    "                        batch_dict[key] += [value]\n",
    "            \n",
    "            for key, value in batch_dict.items():\n",
    "                batch_dict[key] = np.array(value)\n",
    "                \n",
    "            '''print(f\"batch_idx: {batch_idx}, sample_batch: {batch_dict.keys()}\")\n",
    "            \n",
    "            for key,value in batch_dict.items():\n",
    "                print(f\"key: {value.shape}\")'''\n",
    "if 0:\n",
    "    get_data_from_dataset(phosc_loader.trainset, \n",
    "                          3, \n",
    "                          math.ceil(trainset_sz/loader_config_value['batch_size']), \n",
    "                          loader_config_value['batch_size'])\n",
    "    get_data_from_dataset(phosc_loader.validset, \n",
    "                          3, \n",
    "                          math.ceil(trainset_sz/loader_config_value['batch_size']), \n",
    "                          loader_config_value['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a88ddde-480c-434b-bb47-fb36de7e3f5a",
   "metadata": {},
   "source": [
    "# Test <code>loader</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bfd82ad-25bc-47ce-9c39-7d9aca6274ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a Batch of DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SHOW_INPUT=False\n",
    "if SHOW_INPUT:\n",
    "    plt.figure(figsize = (8,16))\n",
    "    fig, axes = plt.subplots(2, 5) # factors of batch_size\n",
    "    for batch_idx, batch in enumerate(phosc_loader.train_dataloader()):\n",
    "        for key, value in batch.items():\n",
    "            print(f\"idx: {batch_idx}, key: {key}, shape: {value.shape}\")\n",
    "\n",
    "        #for e,(img, lbl) in enumerate(zip(images, labels)):\n",
    "        images = (torch.permute(batch['img'], (0, 2, 3, 1))*255.0).numpy().astype('uint8')\n",
    "        labels = phosc_loader.wordLabelEncoder.inverse_transform(batch['wlabel'].numpy().tolist())\n",
    "        print(labels)\n",
    "        for idx in range(images.shape[0]):\n",
    "            img = images[idx, :, :, :]\n",
    "\n",
    "            plt.subplot(2, 5, idx+1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'{labels[idx]}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        val = 'y' #0\n",
    "        while val != 'y':\n",
    "            val = input(\"Enter y to continue: \")\n",
    "\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664c064-865c-493e-939e-601fcbf787d1",
   "metadata": {},
   "source": [
    "# Test <code>model</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bdb6b-8f2c-467a-a418-e6b095fc1e25",
   "metadata": {},
   "source": [
    "## Print <code>model</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc9d15fa-3ffd-4556-b2b4-5f73c6c182cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_in_ch': 3, 'conv_fmaps': [64, 128, 256, 256, 256, 512], 'conv_kernel_size': 3, 'conv_stride': 1, 'conv_padding': 1, 'conv_layer_order': 'cr', 'conv_last_single_conv_fmap': 512, 'pool_layer_index': [1, 3], 'pool_kernel_size': 2, 'pool_stride': 2, 'pool_padding': 0, 'pyramid_pooling_type': 'spatial', 'pyramid_pooling_levels': [1, 2, 4], 'phos_head_fmaps': [4096, 4096, 165], 'phos_head_dropout': [0.5, 0.5], 'phoc_head_fmaps': [4096, 4096, 604], 'phoc_head_dropout': [0.5, 0.5]}\n"
     ]
    }
   ],
   "source": [
    "def append_key_to_subconfig(main_key, subconfig):\n",
    "    config = {}\n",
    "    for key, value in subconfig.items():\n",
    "        new_key = '_'.join([main_key, key])\n",
    "        config[new_key] = value\n",
    "    return config\n",
    "\n",
    "_model_config = next(iter(args['model'].values()))\n",
    "model_config = {}\n",
    "    \n",
    "for key, value in _model_config.items():\n",
    "    _config = append_key_to_subconfig(key, value)\n",
    "    model_config.update(_config)\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e46ac9b-d934-4e56-aaa0-f83fd01b81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.phocnet.phosc import PHOSCNet\n",
    "conv_in_ch = model_config.pop('conv_in_ch')\n",
    "conv_fmaps = model_config.pop('conv_fmaps')\n",
    "model = PHOSCNet(conv_in_ch, conv_fmaps, **model_config)\n",
    "\n",
    "PRINT_MODEL=False\n",
    "if PRINT_MODEL:\n",
    "    print(model.cnn_arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a7675-b655-422a-b0d5-9c4caf209a25",
   "metadata": {},
   "source": [
    "## Run <code>model</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68adf2cd-ef3d-4dff-ad35-37dcfa6f312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODEL=False\n",
    "if RUN_MODEL:\n",
    "    for batch_idx, batch in enumerate(phosc_loader.train_dataloader()):\n",
    "        cnn_feats = model.forward(batch['img'])\n",
    "        print(cnn_feats['phos'].shape, cnn_feats['phoc'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e83a1b6-d742-4166-8385-10dfdaa9eb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'losses': {'phos': {'weight': 1.5, 'pred_idx': 'phos', 'target_idx': 'phos', 'function': 'models.loss.phosc.PhosLoss'}, 'phoc': {'weight': 4.5, 'pred_idx': 'phoc', 'target_idx': 'phoc', 'function': 'models.loss.phosc.PhocLoss'}}, 'sum_loss': {'grad_stats': ['norm', 'max', 'mean'], 'split_pred': True, 'split_target': True}}\n"
     ]
    }
   ],
   "source": [
    "criterion_config = args['criterion']\n",
    "print(criterion_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cf759-5d80-4355-8198-70b0383070af",
   "metadata": {},
   "source": [
    "# Test <code>loss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71027e2b-c4aa-48d7-979e-7f8df1e7b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.loss.sumloss import SumLoss\n",
    "assert 'sum_loss' in criterion_config.keys(), \"Error: Include sum_loss config in config (required of single loss case)\"\n",
    "sum_loss_cls = SumLoss(criterion_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca025e6-62df-45bb-9f70-ce56a9d278da",
   "metadata": {},
   "source": [
    "## Run <code>loss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1a19325-825f-46b2-8685-978c4c81292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODEL_WITH_LOSS=False\n",
    "if RUN_MODEL_WITH_LOSS:\n",
    "    for batch_idx, batch in enumerate(phosc_loader.train_dataloader()):\n",
    "            #xx = torch.permute(batch['img'], (0, 3, 1, 2)).float()/255.0\n",
    "            cnn_feats = model.forward(batch['img'])\n",
    "            #print(cnn_feats['phos'].shape, cnn_feats['phoc'].shape)\n",
    "            loss = sum_loss_cls(cnn_feats, batch)\n",
    "            print(f\"batch[{batch_idx}], loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261d668-d643-46ad-8710-f7e695221afc",
   "metadata": {},
   "source": [
    "# Test <code>metric</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e85dbed-7da6-45a6-a78b-a5edbd4a997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchmetrics import CosineSimilarity\n",
    "#from models.metric.wsmetric import WSMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf9665-9a32-4d8b-bd19-14057cc16236",
   "metadata": {},
   "source": [
    "# Run <code>metric</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eededdd-0514-4f6c-86d6-a060488e4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "def log_plots(img_batch, pred_word, target_word):\n",
    "    N_COLS = 5\n",
    "    N_ROWS = math.ceil(img_batch.shape[0]/N_COLS)\n",
    "    SCALE = 2\n",
    "    FIG_HT, FIG_WD = 4, 6\n",
    "    plt.figure()\n",
    "    fig, axes = plt.subplots(N_ROWS, N_COLS, figsize = (SCALE*FIG_WD, SCALE*FIG_HT)) # factors of batch_size\n",
    "    for idx in range(img_batch.shape[0]):\n",
    "        img = img_batch[idx, :, :, :]\n",
    "\n",
    "        _fig = plt.subplot(N_ROWS, N_COLS, idx+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'({pred_word[idx]}, {target_word[idx]})', fontsize=15)\n",
    "        #_fig.text(5, 1, f'{pred_word[idx]}', ha=\"center\", va=\"bottom\", fontsize=15, color=\"blue\")\n",
    "        #_fig.text(15, 1, f'{target_word[idx]}', ha=\"center\", va=\"bottom\", fontsize=15, color=\"green\")\n",
    "        \n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Now we can save it to a numpy array.\n",
    "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    \n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2891b5-e1dc-419b-a771-8fb12e4dbd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalyan/gitRepos/projects/academic/2021/wszshot/models/metric/wsmetric.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272126608/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  self.word_feature_map[column] = torch.tensor(word_feature_map[column]).cuda()\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from utils.generic.getaccess import get_class_from_str\n",
    "\n",
    "class_fn = get_class_from_str(args['metric']['name'])\n",
    "ws_metric = class_fn(phosc_loader.df_all, phosc_loader.wordLabelEncoder)\n",
    "\n",
    "RUN_METRIC_ON_BATCH = False\n",
    "if RUN_METRIC_ON_BATCH:\n",
    "    for batch_idx, batch in enumerate(phosc_loader.train_dataloader()):\n",
    "        pred   = deepcopy(batch)\n",
    "        target = deepcopy(batch)\n",
    "        img_batch = target.pop('img')\n",
    "        img_batch = (torch.permute(img_batch, (0, 2, 3, 1))*255.0).numpy().astype('uint8')\n",
    "        _metric = ws_metric.compute(pred, target)\n",
    "        \n",
    "        '''disp_img = log_image_tiles(img_batch, _metric['accuracy_word'].tolist(), \n",
    "                        phosc_loader.wordLabelEncoder.inverse_transform(target['wlabel'].tolist()))'''\n",
    "        disp_img = log_plots(img_batch, _metric['accuracy_word'].tolist(), \n",
    "                        phosc_loader.wordLabelEncoder.inverse_transform(target['wlabel'].tolist()))\n",
    "        im = Image.fromarray(disp_img)\n",
    "        im.save(f\"your_file_{batch_idx}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8414e-5eb0-46b8-bf71-80fa9868857f",
   "metadata": {},
   "source": [
    "# Setup <code>LightningModule</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e1fa8e9-46c2-4238-b4de-947b8c23f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "class PhoscWSTask(LightningModule):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 loss,\n",
    "                 metric,\n",
    "                 trainer_config):\n",
    "        super().__init__()\n",
    "        #pdb.set_trace()\n",
    "        self.model  = model\n",
    "        self.loss   = loss\n",
    "        self.metric = metric\n",
    "        \n",
    "        self.num_tile_col =  trainer_config['num_tile_col']\n",
    "        self.log_train_every = trainer_config['intervals']['log_train_every']\n",
    "        self.log_valid_every = trainer_config['intervals']['log_valid_every']\n",
    "        self.validate_every = trainer_config['intervals']['validate_every']\n",
    "        self.optimizer = trainer_config['optimizer']\n",
    "        assert 'name' in self.optimizer.keys(), 'Error: Optimizer needs to be selected'\n",
    "        assert self.optimizer['name'] == 'Adam', 'Error: Only Adam optimizer implemeted in trainer'\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #pdb.set_trace()\n",
    "        logging = self.global_step % self.log_train_every == 0\n",
    "        dict_values = self._shared_step(batch, batch_idx, logging)\n",
    "        ret_values  = self._populate_return_values(dict_values)\n",
    "        self._populate_tensorboard_logs(ret_values, stage='train', logging=logging)\n",
    "        \n",
    "        step_dict={\n",
    "            # required\n",
    "            'loss': ret_values['loss']}\n",
    "        print(\"At training_step, global_step=\", self.global_step)\n",
    "        return step_dict \n",
    "        \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #pdb.set_trace()\n",
    "        logging = self.global_step % self.log_train_every == 0\n",
    "        dict_values = self._shared_step(batch, batch_idx, logging)\n",
    "        ret_values  = self._populate_return_values(dict_values)\n",
    "        self._populate_tensorboard_logs(ret_values, stage='validation', logging=logging)\n",
    "        \n",
    "        step_dict={\n",
    "            # required\n",
    "            'loss': ret_values['loss']}\n",
    "    \n",
    "        print(\"At validation_step, global_step=\", self.global_step)\n",
    "        return step_dict \n",
    "        \n",
    "        \n",
    "    def _shared_step(self, batch, batch_idx, logging):\n",
    "        img = batch.pop('img')\n",
    "        cnn_feats = self.model(img)\n",
    "        loss = self.loss(cnn_feats, batch)\n",
    "        acc = self.metric.compute(cnn_feats, batch)\n",
    "        if logging:\n",
    "            img_batch = (torch.permute(img, (0, 2, 3, 1))*255.0).cpu().numpy().astype('uint8')\n",
    "            tiled_img = self._log_image_tiles(img_batch, acc['accuracy_word'].tolist(), \n",
    "                        self.metric.wordLabelEncoder.inverse_transform(batch['wlabel'].tolist()))\n",
    "        else:\n",
    "            tiled_img = None\n",
    "        \n",
    "        return {'loss': loss, \n",
    "                'accuracy': acc, \n",
    "                'prediction': tiled_img}\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.optimizer['lr'], betas=self.optimizer['betas'])\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        dict_values = self._shared_step(batch, batch_idx, True)\n",
    "        ret_values  = self._populate_return_values(dict_values)\n",
    "        self._populate_tensorboard_logs(ret_values, stage='test')\n",
    "        \n",
    "        step_dict={\n",
    "            # required\n",
    "            'loss': ret_values['loss']}\n",
    "    \n",
    "        print(\"At test_step, global_step=\", self.global_step)\n",
    "        return step_dict \n",
    "        \n",
    "\n",
    "    '''\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def _log_image_tiles(self, img_batch, pred_word, target_word):\n",
    "        N_COLS = self.num_tile_col\n",
    "        N_ROWS = math.ceil(img_batch.shape[0]/N_COLS)\n",
    "        SCALE = 2\n",
    "        FIG_HT, FIG_WD = 4, 6\n",
    "\n",
    "        plt.figure()\n",
    "        fig, axes = plt.subplots(N_ROWS, N_COLS, figsize = (SCALE*FIG_WD, SCALE*FIG_HT)) # factors of batch_size\n",
    "        for idx in range(img_batch.shape[0]):\n",
    "            img = img_batch[idx, :, :, :]\n",
    "\n",
    "            _fig = plt.subplot(N_ROWS, N_COLS, idx+1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'({pred_word[idx]}, {target_word[idx]})', fontsize=15)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        # Now we can save it to a numpy array.\n",
    "        np_arr = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        np_arr = np_arr.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "        plt.cla()\n",
    "        plt.close('all')\n",
    "        return np_arr\n",
    "\n",
    "    def _populate_return_values(self,\n",
    "                               dict_values):\n",
    "        ret_values = {}\n",
    "        for key, value in dict_values.items():\n",
    "            if key == 'accuracy': #accuracy returns a dict of values\n",
    "                for _key, _value in value.items():\n",
    "                    if _key != 'accuracy_word':\n",
    "                        ret_values[_key] = _value\n",
    "            else:\n",
    "                ret_values[key] = value\n",
    "        \n",
    "        return ret_values\n",
    "        \n",
    "    def _populate_tensorboard_logs(self,\n",
    "                                   ret_values,\n",
    "                                   stage='train',\n",
    "                                   logging=False):\n",
    "        ''' ret_values keys : \n",
    "        scalars: 'loss', 'similarity_phos', 'similarity_phoc', 'similarity_phosc', 'accuracy_phos', 'accuracy_phoc', 'accuracy_phosc'\n",
    "        image: 'prediction'\n",
    "        '''\n",
    "        # logging using tensorboard logger\n",
    "        if logging:\n",
    "            for key, value in ret_values.items():\n",
    "                if key == 'prediction':\n",
    "                    self.logger.experiment.add_image(f'{stage}_{key}', value, self.global_step, dataformats=\"HWC\")\n",
    "                else:\n",
    "                    self.logger.experiment.add_scalar(f'{stage}_{key}', value, self.global_step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedebffd-da68-485f-8777-e609e63a93c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/home/kalyan/anaconda3/envs/deepl/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | PHOSCNet | 9.4 M \n",
      "1 | loss  | SumLoss  | 0     \n",
      "-----------------------------------\n",
      "9.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.4 M     Total params\n",
      "37.622    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalyan/gitRepos/projects/academic/2021/wszshot/models/metric/wsmetric.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  phos = torch.mean(torch.tensor(phos_wvec == target['wlabel'], dtype=torch.float)).item()\n",
      "/home/kalyan/gitRepos/projects/academic/2021/wszshot/models/metric/wsmetric.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  phoc = torch.mean(torch.tensor(phoc_wvec == target['wlabel'], dtype=torch.float)).item()\n",
      "/home/kalyan/gitRepos/projects/academic/2021/wszshot/models/metric/wsmetric.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  phosc = torch.mean(torch.tensor(phosc_wvec == target['wlabel'], dtype=torch.float)).item()\n",
      "/home/kalyan/anaconda3/envs/deepl/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Tight layout not applied. tight_layout cannot make axes height small enough to accommodate all axes decorations\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At validation_step, global_step= 0\n",
      "At training_step, global_step= 0\n",
      "At training_step, global_step= 1\n",
      "At training_step, global_step= 2\n",
      "At training_step, global_step= 3\n",
      "At training_step, global_step= 4\n",
      "At training_step, global_step= 5\n",
      "At training_step, global_step= 6\n",
      "At training_step, global_step= 7\n",
      "At training_step, global_step= 8\n",
      "At training_step, global_step= 9\n",
      "At training_step, global_step= 10\n",
      "At training_step, global_step= 11\n",
      "At training_step, global_step= 12\n",
      "At training_step, global_step= 13\n",
      "At training_step, global_step= 14\n",
      "At training_step, global_step= 15\n",
      "At training_step, global_step= 16\n",
      "At training_step, global_step= 17\n",
      "At training_step, global_step= 18\n",
      "At training_step, global_step= 19\n",
      "At training_step, global_step= 20\n",
      "At training_step, global_step= 21\n",
      "At training_step, global_step= 22\n",
      "At training_step, global_step= 23\n",
      "At training_step, global_step= 24\n",
      "At training_step, global_step= 25\n",
      "At training_step, global_step= 26\n",
      "At training_step, global_step= 27\n",
      "At training_step, global_step= 28\n",
      "At training_step, global_step= 29\n",
      "At training_step, global_step= 30\n",
      "At training_step, global_step= 31\n",
      "At validation_step, global_step= 31\n",
      "At training_step, global_step= 32\n",
      "At training_step, global_step= 33\n",
      "At training_step, global_step= 34\n",
      "At training_step, global_step= 35\n",
      "At training_step, global_step= 36\n",
      "At training_step, global_step= 37\n",
      "At training_step, global_step= 38\n",
      "At training_step, global_step= 39\n",
      "At training_step, global_step= 40\n",
      "At training_step, global_step= 41\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if 1:\n",
    "    seed_everything(42)\n",
    "    trainer_config = args['trainer']\n",
    "    pl_phosc_model = PhoscWSTask(model,\n",
    "                 sum_loss_cls,\n",
    "                 ws_metric,\n",
    "                 trainer_config)\n",
    "    phosc_logger = TensorBoardLogger(\"tb_logs\", name=\"ws_phosc_default\")\n",
    "    trainer = Trainer(gpus=1,\n",
    "                      limit_val_batches=1,\n",
    "                      val_check_interval=trainer_config['intervals']['validate_every'],\n",
    "                      progress_bar_refresh_rate=0,\n",
    "                      max_epochs=trainer_config['max_epochs'],\n",
    "                      logger=phosc_logger)\n",
    "    trainer.fit(pl_phosc_model, phosc_loader.train_dataloader(), phosc_loader.val_dataloader())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
